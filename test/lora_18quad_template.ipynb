{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83d\ude80 18QUAD LoRA \u5b66\u7fd2\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\n", "\u3053\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3067\u306f\u3001LoRA\u3092\u4f7f\u3063\u306618QUAD\u306b\u7279\u5316\u3057\u305f\u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u5fae\u8abf\u6574\u3092\u884c\u3044\u307e\u3059\u3002"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u2705 \u5fc5\u8981\u306a\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n", "!pip install -q transformers datasets peft accelerate bitsandbytes"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u2705 \u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\uff08JSONL\u5f62\u5f0f\uff09\n", "from datasets import load_dataset\n", "dataset = load_dataset('json', data_files='18quad_lora_train.jsonl', split='train')\n", "dataset = dataset.train_test_split(test_size=0.1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u2705 \u30e2\u30c7\u30eb\u3068LoRA\u8a2d\u5b9a\u306e\u8aad\u307f\u8fbc\u307f\n", "from transformers import AutoTokenizer, AutoModelForCausalLM\n", "from peft import get_peft_model, LoraConfig, TaskType\n", "\n", "model_name = 'meta-llama/Llama-2-7b-hf'  # or any other compatible model\n", "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n", "model = AutoModelForCausalLM.from_pretrained(model_name, load_in_8bit=True, device_map='auto')\n", "\n", "peft_config = LoraConfig(\n", "    task_type=TaskType.CAUSAL_LM,\n", "    inference_mode=False,\n", "    r=8,\n", "    lora_alpha=32,\n", "    lora_dropout=0.1\n", ")\n", "model = get_peft_model(model, peft_config)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u2705 \u5b66\u7fd2\u6e96\u5099\u3068Trainer\u8a2d\u5b9a\n", "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n", "\n", "training_args = TrainingArguments(\n", "    output_dir=\"./lora-18quad\",\n", "    per_device_train_batch_size=4,\n", "    gradient_accumulation_steps=2,\n", "    num_train_epochs=3,\n", "    logging_dir=\"./logs\",\n", "    logging_steps=10,\n", "    save_total_limit=2,\n", "    save_steps=100,\n", "    evaluation_strategy=\"steps\",\n", "    eval_steps=100,\n", "    fp16=True,\n", "    report_to=\"none\"\n", ")\n", "\n", "trainer = Trainer(\n", "    model=model,\n", "    args=training_args,\n", "    train_dataset=dataset['train'],\n", "    eval_dataset=dataset['test'],\n", "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n", ")\n", "trainer.train()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 2}